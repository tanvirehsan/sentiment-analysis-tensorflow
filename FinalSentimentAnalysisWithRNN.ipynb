{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Copy N of SentimentAnalysisWithRNN.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7YngCEet1Vs",
        "colab_type": "text"
      },
      "source": [
        "# Project: \"Sentiment Analysis from Review\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-VNVARmgISj",
        "colab_type": "text"
      },
      "source": [
        "# Getting Our Machine Ready"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXtbCtUfWF6G",
        "colab_type": "code",
        "outputId": "fd4d0438-fcb5-43ea-acf5-931d6c45feb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk, re, time\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from collections import namedtuple"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q6B9SHEuySj",
        "colab_type": "text"
      },
      "source": [
        "#Data Set Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc68igHxWF6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data\n",
        "train_data = pd.read_csv(\"https://raw.githubusercontent.com/tanvirehsan/sentiment-analysis-tensorflow/master/Data/word2vec-nlp-tutorial/labeledTrainData.tsv\", delimiter=\"\\t\")\n",
        "test_data = pd.read_csv(\"https://raw.githubusercontent.com/tanvirehsan/sentiment-analysis-tensorflow/master/Data/word2vec-nlp-tutorial/testData.tsv\", delimiter=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aga5IpC7WF6N",
        "colab_type": "text"
      },
      "source": [
        "# Data Inspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MjhK4n7WF6O",
        "colab_type": "code",
        "outputId": "ad8b1ec5-9472-4159-ff8d-e1aa11e06645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Print Training Data\n",
        "train_data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5814_8</td>\n",
              "      <td>1</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2381_9</td>\n",
              "      <td>1</td>\n",
              "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7759_3</td>\n",
              "      <td>0</td>\n",
              "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3630_4</td>\n",
              "      <td>0</td>\n",
              "      <td>It must be assumed that those who praised this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9495_8</td>\n",
              "      <td>1</td>\n",
              "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  sentiment                                             review\n",
              "0  5814_8          1  With all this stuff going down at the moment w...\n",
              "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
              "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
              "3  3630_4          0  It must be assumed that those who praised this...\n",
              "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlLzzAzXWF6S",
        "colab_type": "code",
        "outputId": "c8623ff5-27d4-4480-ee2d-d2f930779d64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Print Test Data\n",
        "test_data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12311_10</td>\n",
              "      <td>Naturally in a film who's main themes are of m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8348_2</td>\n",
              "      <td>This movie is a disaster within a disaster fil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5828_4</td>\n",
              "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7186_2</td>\n",
              "      <td>Afraid of the Dark left me with the impression...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12128_7</td>\n",
              "      <td>A very accurate depiction of small time mob li...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                             review\n",
              "0  12311_10  Naturally in a film who's main themes are of m...\n",
              "1    8348_2  This movie is a disaster within a disaster fil...\n",
              "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
              "3    7186_2  Afraid of the Dark left me with the impression...\n",
              "4   12128_7  A very accurate depiction of small time mob li..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CPCmAzUWF6W",
        "colab_type": "code",
        "outputId": "31a68c4d-baab-46a6-d0b7-580090e83d4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Printing shape of Data\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 3)\n",
            "(25000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhOXdh-CWF6d",
        "colab_type": "code",
        "outputId": "5a5b1bd8-aa0f-4fbc-baa2-621152347d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Inspect first 3 reviews\n",
        "for i in range(3):\n",
        "    print(train_data.review[i])\n",
        "    print()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\n",
            "\n",
            "\\The Classic War of the Worlds\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\"critics\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\"critics\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\"critics\\\" perceive to be its shortcomings.\"\n",
            "\n",
            "The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.<br /><br />The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : ¨Sabretooth(2002)¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¨10.000 BC(2006)¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhBMNbxNWF6f",
        "colab_type": "code",
        "outputId": "cc665420-1d27-4168-9dcf-2a8522cdffdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Check for any null values\n",
        "print(train_data.isnull().sum())\n",
        "print(test_data.isnull().sum())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id           0\n",
            "sentiment    0\n",
            "review       0\n",
            "dtype: int64\n",
            "id        0\n",
            "review    0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcs30aOqWF6i",
        "colab_type": "text"
      },
      "source": [
        "# Method for Cleaning and Format Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7naw63YRWF6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanText(text, remove_stopwords=True):\n",
        "    '''Clean the text, with the option to remove stopwords'''\n",
        "    \n",
        "    # Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        #print(stops)    \n",
        "        text = [w for w in text if not w in stops]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"<br />\", \" \", text)\n",
        "    text = re.sub(r\"[^a-z]\", \" \", text)\n",
        "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
        "    text = re.sub(r\"  \", \" \", text)\n",
        "    \n",
        "    # Remove punctuation from text\n",
        "    text = ''.join([c for c in text if c not in punctuation])\n",
        "    \n",
        "    # Return a list of words\n",
        "    return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7wGonGoWF6o",
        "colab_type": "text"
      },
      "source": [
        "#Clean the training DataSet and testing DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3WTMTfAWF6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cleaning Training DataSet using the cleanText() method\n",
        "trainData_clean = []\n",
        "for review in train_data.review:\n",
        "    trainData_clean.append(cleanText(review))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZqxG5Ir9L28",
        "colab_type": "code",
        "outputId": "b27f8dd0-7738-4be6-8334-b3d86afb2bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Inspect the top 3 cleaned Train Dataset\n",
        "for i in range(3):\n",
        "    print(trainData_clean[i])\n",
        "    print()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stuff going moment mj i ve started listening music watching odd documentary there watched wiz watched moonwalker again maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj s feeling towards press also obvious message drugs bad m kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice him the actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond me mj overheard plans nah joe pesci s character ranted wanted people know supplying drugs etc dunno maybe hates mj s music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another which think people not stay away try give wholesome message ironically mj s bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention i ve gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter \n",
            "\n",
            " the classic war worlds timothy hines entertaining film obviously goes great effort lengths faithfully recreate h g wells classic book mr hines succeeds so i watched film me appreciated fact standard predictable hollywood fare comes every year e g spielberg version tom cruise slightest resemblance book obviously everyone looks different things movie envision amateur critics look criticize everything can others rate movie important bases like entertained people never agree critics enjoyed effort mr hines put faithful h g wells classic novel found entertaining made easy overlook critics perceive shortcomings \n",
            "\n",
            "film starts manager nicholas bell giving welcome investors robert carradine primal park secret project mutating primal animal using fossilized dna like jurassik park scientists resurrect one nature s fearsome predators sabretooth tiger smilodon scientific ambition turns deadly however high voltage fence opened creature escape begins savagely stalking prey human visitors tourists scientific meanwhile youngsters enter restricted area security center attacked pack large pre historical animals deadlier bigger addition security agent stacy haiduk mate brian wimmer fight hardly carnivorous smilodons sabretooths course real star stars astounding terrifyingly though convincing giant animals savagely stalking prey group run afoul fight one nature s fearsome predators furthermore third sabretooth dangerous slow stalks victims the movie delivers goods lots blood gore beheading hair raising chills full scares sabretooths appear mediocre special effects the story provides exciting stirring entertainment results quite boring the giant animals majority made computer generator seem totally lousy middling performances though players reacting appropriately becoming food actors give vigorously physical performances dodging beasts running bound leaps dangling walls packs ridiculous final deadly scene small kids realistic gory violent attack scenes films sabretooths smilodon following sabretooth  by james r hickox vanessa angel david keith john rhys davies much better  bc  roland emmerich steven strait cliff curtis camilla belle motion picture filled bloody moments badly directed george miller originality takes many elements previous films miller australian director usually working television tidal wave journey center earth many others occasionally cinema man snowy river zeus roxanne robinson crusoe rating average bottom barrel \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Lo7CD4EWF6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cleaning Test DataSet using the cleanText() method\n",
        "testData_clean = []\n",
        "for review in test_data.review:\n",
        "    testData_clean.append(cleanText(review))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJuZgNy0WF6w",
        "colab_type": "code",
        "outputId": "bcf10887-0888-46aa-b823-45faa206f4c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Inspect the top 3 cleaned Test Dataset\n",
        "for i in range(3):\n",
        "    print(testData_clean[i])\n",
        "    print()\n",
        "    \n",
        "print(len(trainData_clean))\n",
        "print(len(testData_clean))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "naturally film who s main themes mortality nostalgia loss innocence perhaps surprising rated highly older viewers younger ones however craftsmanship completeness film anyone enjoy pace steady constant characters full engaging relationships interactions natural showing need floods tears show emotion screams show fear shouting show dispute violence show anger naturally joyce s short story lends film ready made structure perfect polished diamond small changes huston makes inclusion poem fit neatly truly masterpiece tact subtlety overwhelming beauty \n",
            "\n",
            "movie disaster within disaster film full great action scenes meaningful throw away sense reality let s see word wise lava burns you steam burns you can t stand next lava diverting minor lava flow difficult let alone significant one scares think might actually believe saw movie even worse significant amount talent went making film mean acting actually good effects average hard believe somebody read scripts allowed talent wasted guess suggestion would movie start tv  look away like train wreck awful know coming watch look away spend time meaningful content \n",
            "\n",
            "all movie kids saw tonight child loved it one point kid s excitement great sitting impossible however great fan a a milne s books subtle hide wry intelligence behind childlike quality leading characters film subtle seems shame disney cannot see benefit making movies stories contained pages although perhaps permission use them found wishing theater replaying winnie the pooh tigger too instead characters voices good really bothered kanga music however twice loud parts dialog incongruous film as story bit preachy militant tone overall disappointed would go see excitement child s face i liked lumpy s laugh  \n",
            "\n",
            "25000\n",
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofD7rrw9WF6z",
        "colab_type": "code",
        "outputId": "c4eb2198-5e8f-4518-bfdf-9d140a9f692d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Combine All review Data (Cleaned)\n",
        "combined_all_cleaned_reviews = trainData_clean + testData_clean\n",
        "# Tokenize the reviews\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(combined_all_cleaned_reviews)\n",
        "print(\"Fitting is complete.\")\n",
        "\n",
        "trainData_seq = tokenizer.texts_to_sequences(trainData_clean)\n",
        "print(\"Text to Sequence conversion is complete for training data.\")\n",
        "\n",
        "testData_seq = tokenizer.texts_to_sequences(testData_clean)\n",
        "print(\"Text to Sequence conversion is complete for test data\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting is complete.\n",
            "Text to Sequence conversion is complete for training data.\n",
            "Text to Sequence conversion is complete for test data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpL2XNr8WF62",
        "colab_type": "code",
        "outputId": "da2ae7ac-a357-4c42-d877-bf9cba68847a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Find the number of unique tokens\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Words in index: %d\" % len(word_index))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words in index: 99425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6LvGYL4WF66",
        "colab_type": "code",
        "outputId": "904cf940-1830-4f22-bdec-324cd3394e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Inspect Top 3 Trainig reviews(Cleaned) after they have been tokenized\n",
        "for i in range(3):\n",
        "    print(trainData_seq[i])\n",
        "    print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[437, 81, 481, 10863, 6, 71, 573, 2590, 115, 65, 948, 551, 51, 207, 24383, 207, 17034, 213, 188, 92, 19, 684, 2550, 118, 104, 14, 511, 3933, 188, 25, 240, 644, 2336, 1251, 17034, 85, 4772, 85, 701, 3, 298, 81, 15, 351, 1827, 533, 1209, 3566, 10863, 1, 477, 861, 3526, 22, 517, 662, 1403, 18, 60, 5290, 2073, 1109, 180, 406, 1512, 807, 2559, 5, 10863, 469, 81, 655, 80, 265, 109, 569, 10863, 33435, 29469, 141, 2, 10863, 374, 12, 57, 24, 374, 205, 14, 246, 173, 9, 740, 701, 3, 135, 334, 456, 138, 16333, 4102, 1702, 626, 865, 10467, 1009, 11946, 880, 1058, 1640, 408, 10863, 258, 18, 584, 134, 10863, 17899, 2288, 15681, 865, 10467, 1, 32, 36026, 381, 20, 46, 17447, 1403, 426, 9779, 188, 4220, 10863, 1, 115, 658, 511, 91, 5, 10863, 1541, 436, 2257, 131, 2124, 2367, 626, 22, 67, 112, 4731, 5337, 300, 1313, 29470, 18, 626, 547, 878, 655, 685, 4, 444, 190, 537, 131, 677, 3371, 1224, 779, 54, 1229, 260, 2, 20, 5, 10863, 4, 570, 73, 468, 30, 20, 239, 695, 151, 269, 108, 7617, 662, 3514, 10863, 1, 36027, 1676, 2, 152, 406, 1512, 287, 4, 946, 20, 48, 1488, 1215, 2336, 16, 601, 6, 71, 434, 713, 7167, 16, 46, 20, 194, 435, 3890, 3466, 46, 105, 263, 487, 246, 282, 118, 4, 19320, 19321, 355, 1489]\n",
            "\n",
            "[9, 272, 212, 3549, 4509, 10468, 348, 3, 452, 183, 21, 697, 8411, 10998, 8646, 1850, 1081, 4599, 272, 184, 359, 10468, 2736, 177, 6, 207, 3, 134, 2507, 105, 1190, 637, 262, 2544, 187, 84, 208, 669, 1081, 3317, 232, 746, 3719, 3878, 4315, 184, 452, 209, 203, 194, 91, 2, 19917, 2368, 1337, 79, 6462, 181, 87, 317, 884, 2, 567, 12471, 5, 2241, 20, 40, 941, 1337, 431, 697, 359, 10468, 186, 2779, 1850, 1081, 4599, 272, 591, 167, 348, 24, 680, 5218, 1337, 10999, 5607]\n",
            "\n",
            "[3, 456, 2875, 4773, 3612, 659, 2478, 17035, 528, 4475, 10238, 1147, 928, 1053, 31269, 10238, 1626, 707, 50267, 10239, 5, 50268, 1147, 3015, 11298, 4, 826, 1, 17900, 9575, 14531, 5037, 43715, 3623, 5338, 432, 2413, 102, 214, 26605, 7112, 2930, 1469, 962, 751, 17036, 6288, 4709, 308, 9361, 8885, 3623, 1933, 7431, 2248, 11138, 1524, 2479, 2087, 2931, 3016, 949, 1709, 1242, 1513, 33436, 1851, 1594, 2479, 1395, 9576, 36028, 4020, 1686, 50269, 476, 951, 23473, 62125, 31270, 180, 63, 237, 321, 5665, 20502, 70, 1009, 1253, 1513, 17036, 6288, 4709, 463, 412, 16334, 476, 4, 826, 1, 17900, 9575, 3791, 791, 14531, 1661, 480, 9577, 1450, 9, 2, 1467, 6243, 658, 464, 520, 17037, 1021, 5060, 5630, 279, 2668, 31270, 896, 1496, 222, 202, 9, 13, 1525, 1064, 8047, 639, 1936, 93, 265, 9, 1253, 1513, 2177, 24, 1213, 19918, 226, 377, 2198, 21883, 285, 70, 1800, 12472, 5083, 1530, 1596, 66, 108, 19919, 1646, 285, 16676, 12657, 554, 2685, 9163, 12855, 3613, 6966, 564, 388, 2413, 54, 316, 264, 722, 1994, 1046, 1275, 58, 33, 31270, 43715, 888, 14531, 959, 523, 1166, 31271, 6498, 2190, 512, 4021, 223, 8183, 3842, 17, 52, 16335, 9900, 36029, 2025, 16677, 3994, 4092, 8113, 6499, 1217, 353, 983, 1627, 304, 845, 442, 645, 2651, 2686, 221, 36, 730, 848, 33, 2651, 2038, 67, 547, 685, 608, 22662, 3105, 1170, 2087, 611, 36, 317, 1847, 351, 50, 9258, 1828, 18340, 27876, 4344, 43716, 589, 763, 1229, 5262]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT0JVG00_OOV",
        "colab_type": "code",
        "outputId": "ab7a36a7-ba43-4169-a370-031ab3a00ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Inspect Top 3 Test reviews(Cleaned) after they have been tokenized\n",
        "for i in range(3):\n",
        "    print(testData_seq[i])\n",
        "    print()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1840, 3, 243, 1, 196, 1331, 16836, 4319, 1853, 2898, 302, 1675, 1116, 462, 885, 723, 1041, 587, 102, 13587, 34637, 3, 155, 271, 975, 5520, 1809, 29, 279, 1756, 1428, 5285, 1234, 681, 280, 19332, 1650, 43, 1345, 3849, 43, 1040, 5133, 43, 13113, 489, 43, 2659, 1840, 6978, 1, 250, 13, 6932, 3, 1520, 24, 2400, 320, 5213, 3934, 316, 1267, 4449, 78, 6740, 5427, 1039, 6406, 287, 832, 17683, 3944, 4376, 827]\n",
            "\n",
            "[2, 1494, 666, 1494, 3, 279, 21, 114, 58, 3161, 1283, 151, 198, 531, 190, 1, 15, 576, 1529, 9158, 3051, 168, 6335, 3051, 168, 87, 49, 698, 284, 9158, 18445, 1302, 9158, 2719, 798, 190, 537, 2733, 4, 2668, 30, 140, 75, 175, 120, 2, 11, 349, 2733, 1043, 549, 333, 141, 3, 297, 42, 75, 8, 202, 763, 160, 175, 1693, 254, 3020, 1507, 549, 916, 387, 5937, 12, 2, 292, 145, 79, 151, 5, 988, 3561, 290, 46, 479, 31, 79, 151, 1079, 10, 3161, 1399]\n",
            "\n",
            "[76, 2, 264, 120, 4212, 423, 354, 7, 4, 128, 444, 1, 2343, 21, 1165, 1111, 102, 21, 242, 116, 116, 41843, 1, 1131, 1209, 2380, 10273, 1643, 435, 11326, 399, 905, 29, 3, 1209, 97, 792, 859, 498, 15, 3907, 141, 28, 483, 3995, 5159, 172, 302, 10502, 274, 169, 167, 4669, 689, 20802, 24312, 9, 21703, 38867, 294, 217, 29, 2225, 8, 14, 2439, 81139, 115, 102, 1413, 1260, 439, 647, 12995, 3, 283, 13, 135, 5229, 14639, 1184, 360, 597, 12, 61, 15, 2343, 423, 1, 305, 6, 346, 18068, 1, 340]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyEolgSJWF69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the length of reviews ( Train and Test Dataset)\n",
        "lengths = []\n",
        "for review in trainData_seq:\n",
        "    lengths.append(len(review))\n",
        "\n",
        "for review in testData_seq:\n",
        "    lengths.append(len(review))\n",
        "\n",
        "# Create a dataframe so that the values can be inspected\n",
        "lengths = pd.DataFrame(lengths, columns=['counts'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyANt8ztWF7B",
        "colab_type": "code",
        "outputId": "95ebae6c-9b57-405e-ee57-1596287f26fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "lengths.counts.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    50000.000000\n",
              "mean       132.337460\n",
              "std         99.452039\n",
              "min          3.000000\n",
              "25%         71.000000\n",
              "50%         98.000000\n",
              "75%        161.000000\n",
              "max       1504.000000\n",
              "Name: counts, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDytorkuWF7E",
        "colab_type": "code",
        "outputId": "1a2da5cf-1d79-49aa-cba9-9f3315d2cfd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Percentile functions \n",
        "#To determine the maximum length of review text to be considered \n",
        "print(np.percentile(lengths.counts, 80))\n",
        "print(np.percentile(lengths.counts, 85))\n",
        "print(np.percentile(lengths.counts, 90))\n",
        "print(np.percentile(lengths.counts, 95))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "178.0\n",
            "208.0\n",
            "253.0\n",
            "332.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIATVBz9WF7G",
        "colab_type": "text"
      },
      "source": [
        "**To Train our model faster we are limiting the length of review upto maximum 200.Reviews with more than 200 words will have those extra words removed. Reviews with less than 200 words will have padding tokens added until it reaches the length of 200.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtR290y-WF7H",
        "colab_type": "code",
        "outputId": "2cd66b98-c97b-485b-fee6-b5a774312fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Pad and truncate the review so that they all have the same length.\n",
        "max_review_length = 200\n",
        "\n",
        "trainData_pad = pad_sequences(trainData_seq, maxlen = max_review_length)\n",
        "print(\"Padding of Training Data is complete.\")\n",
        "\n",
        "testData_pad = pad_sequences(testData_seq, maxlen = max_review_length)\n",
        "print(\"Padding of Test Data is complete.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padding of Training Data is complete.\n",
            "Padding of Test Data is complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDuIJ8QtWF7M",
        "colab_type": "code",
        "outputId": "bb77089d-b544-40c1-ce46-7d57f5b657a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# Inspect the Top 3 Train reviews after padding has been completed. \n",
        "for i in range(3):\n",
        "    print(trainData_pad[i,:100])\n",
        "    print()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   85  4772    85   701     3   298    81    15   351  1827   533  1209\n",
            "  3566 10863     1   477   861  3526    22   517   662  1403    18    60\n",
            "  5290  2073  1109   180   406  1512   807  2559     5 10863   469    81\n",
            "   655    80   265   109   569 10863 33435 29469   141     2 10863   374\n",
            "    12    57    24   374   205    14   246   173     9   740   701     3\n",
            "   135   334   456   138 16333  4102  1702   626   865 10467  1009 11946\n",
            "   880  1058  1640   408 10863   258    18   584   134 10863 17899  2288\n",
            " 15681   865 10467     1    32 36026   381    20    46 17447  1403   426\n",
            "  9779   188  4220 10863]\n",
            "\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            "[ 6288  4709   308  9361  8885  3623  1933  7431  2248 11138  1524  2479\n",
            "  2087  2931  3016   949  1709  1242  1513 33436  1851  1594  2479  1395\n",
            "  9576 36028  4020  1686 50269   476   951 23473 62125 31270   180    63\n",
            "   237   321  5665 20502    70  1009  1253  1513 17036  6288  4709   463\n",
            "   412 16334   476     4   826     1 17900  9575  3791   791 14531  1661\n",
            "   480  9577  1450     9     2  1467  6243   658   464   520 17037  1021\n",
            "  5060  5630   279  2668 31270   896  1496   222   202     9    13  1525\n",
            "  1064  8047   639  1936    93   265     9  1253  1513  2177    24  1213\n",
            " 19918   226   377  2198]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoeNqj6EC0K7",
        "colab_type": "code",
        "outputId": "1ad91be6-601a-4f0f-defb-6f92c276b5ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Inspect the Top 3 Test reviews after padding has been completed. \n",
        "for i in range(3):\n",
        "    print(testData_pad[i,:100])\n",
        "    print()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPEgAxzbWF7S",
        "colab_type": "code",
        "outputId": "fe95da70-1057-4458-bcf8-6a7b85083ed3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating the training and validation sets\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(trainData_pad, train_data.sentiment, test_size = 0.15, random_state = 2)\n",
        "x_test = testData_pad\n",
        "print(testData_pad.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIAXT3PDWF7V",
        "colab_type": "code",
        "outputId": "0a10133e-1816-4d23-9a23-072271b98c43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Inspect the shape of the data\n",
        "print(x_train.shape)\n",
        "print(x_valid.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21250, 200)\n",
            "(3750, 200)\n",
            "(25000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5EuPSp6WF7b",
        "colab_type": "text"
      },
      "source": [
        "# Build and Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpqT0d8-WF7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(x, y, batch_size):\n",
        "    '''Create the batches for the training and validation data'''\n",
        "    n_batches = len(x)//batch_size\n",
        "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
        "    for ii in range(0, len(x), batch_size):\n",
        "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLvhKrYNWF7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_batches(x, batch_size):\n",
        "    '''Create the batches for the testing data'''\n",
        "    n_batches = len(x)//batch_size\n",
        "    x = x[:n_batches*batch_size]\n",
        "    for ii in range(0, len(x), batch_size):\n",
        "        yield x[ii:ii+batch_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRFyn1uINiJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_cell(lstm_size, keep_prob):\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
        "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
        "    return drop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "N56ucyI3WF7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_rnn(n_words, embed_size, batch_size, lstm_size, num_layers, \n",
        "              dropout, learning_rate, multiple_fc, fc_units):\n",
        "    '''Build the Recurrent Neural Network'''\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Declare placeholders we'll feed into the graph\n",
        "    with tf.name_scope('inputs'):\n",
        "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
        "\n",
        "    with tf.name_scope('labels'):\n",
        "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
        "\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "\n",
        "    # Create the embeddings\n",
        "    with tf.name_scope(\"embeddings\"):\n",
        "        embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
        "        embed = tf.nn.embedding_lookup(embedding, inputs)\n",
        "\n",
        "    # Build the RNN layers\n",
        "    with tf.name_scope(\"RNN_layers\"):\n",
        "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
        "\n",
        "    # Set the initial state\n",
        "    with tf.name_scope(\"RNN_init_state\"):\n",
        "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "\n",
        "    # Run the data through the RNN layers\n",
        "    with tf.name_scope(\"RNN_forward\"):\n",
        "        outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
        "                                                 initial_state=initial_state)    \n",
        "    \n",
        "    # Create the fully connected layers\n",
        "    with tf.name_scope(\"fully_connected\"):\n",
        "        \n",
        "        # Initialize the weights and biases\n",
        "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
        "        biases = tf.zeros_initializer()\n",
        "        \n",
        "        dense = tf.contrib.layers.fully_connected(outputs[:, -1],\n",
        "                                                  num_outputs = fc_units,\n",
        "                                                  activation_fn = tf.sigmoid,\n",
        "                                                  weights_initializer = weights,\n",
        "                                                  biases_initializer = biases)\n",
        "        dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
        "        \n",
        "        # Depending on the iteration, use a second fully connected layer\n",
        "        if multiple_fc == True:\n",
        "            dense = tf.contrib.layers.fully_connected(dense,\n",
        "                                                      num_outputs = fc_units,\n",
        "                                                      activation_fn = tf.sigmoid,\n",
        "                                                      weights_initializer = weights,\n",
        "                                                      biases_initializer = biases)\n",
        "            dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
        "    \n",
        "    # Make the predictions\n",
        "    with tf.name_scope('predictions'):\n",
        "        predictions = tf.contrib.layers.fully_connected(dense, \n",
        "                                                        num_outputs = 1, \n",
        "                                                        activation_fn=tf.sigmoid,\n",
        "                                                        weights_initializer = weights,\n",
        "                                                        biases_initializer = biases)\n",
        "        tf.summary.histogram('predictions', predictions)\n",
        "    \n",
        "    # Calculate the cost\n",
        "    with tf.name_scope('cost'):\n",
        "        cost = tf.losses.mean_squared_error(labels, predictions)\n",
        "        tf.summary.scalar('cost', cost)\n",
        "    \n",
        "    # Train the model\n",
        "    with tf.name_scope('train'):    \n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "    # Determine the accuracy\n",
        "    with tf.name_scope(\"accuracy\"):\n",
        "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "        tf.summary.scalar('accuracy', accuracy)\n",
        "    \n",
        "    # Merge all of the summaries\n",
        "    merged = tf.summary.merge_all()    \n",
        "\n",
        "    # Export the nodes \n",
        "    export_nodes = ['inputs', 'labels', 'keep_prob', 'initial_state', 'final_state','accuracy',\n",
        "                    'predictions', 'cost', 'optimizer', 'merged']\n",
        "    Graph = namedtuple('Graph', export_nodes)\n",
        "    local_dict = locals()\n",
        "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
        "    \n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1IThfoFWF7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, epochs, log_string):\n",
        "    '''Train the RNN'''\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # Used to determine when to stop the training early\n",
        "        valid_loss_summary = []\n",
        "        \n",
        "        # Keep track of which batch iteration is being trained\n",
        "        iteration = 0\n",
        "\n",
        "        print()\n",
        "        print(\"Training Model: {}\".format(log_string))\n",
        "\n",
        "        train_writer = tf.summary.FileWriter('./logs/3/train/{}'.format(log_string), sess.graph)\n",
        "        valid_writer = tf.summary.FileWriter('./logs/3/valid/{}'.format(log_string))\n",
        "        \n",
        "        vmodel_dir= \"./Review/Model\"\n",
        "        tf.gfile.MkDir(\"Review\")\n",
        "        tf.gfile.MkDir(vmodel_dir)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            state = sess.run(model.initial_state)\n",
        "            \n",
        "            # Record progress with each epoch\n",
        "            train_loss = []\n",
        "            train_acc = []\n",
        "            val_acc = []\n",
        "            val_loss = []\n",
        "\n",
        "            with tqdm(total=len(x_train)) as pbar:\n",
        "                for _, (x, y) in enumerate(get_batches(x_train, y_train, batch_size), 1):\n",
        "                    feed = {model.inputs: x,\n",
        "                            model.labels: y[:, None],\n",
        "                            model.keep_prob: dropout,\n",
        "                            model.initial_state: state}\n",
        "                    summary, loss, acc, state, _ = sess.run([model.merged, \n",
        "                                                             model.cost, \n",
        "                                                             model.accuracy, \n",
        "                                                             model.final_state, \n",
        "                                                             model.optimizer], \n",
        "                                                            feed_dict=feed)                \n",
        "                    \n",
        "                    # Record the loss and accuracy of each training batch\n",
        "                    train_loss.append(loss)\n",
        "                    train_acc.append(acc)\n",
        "                    \n",
        "                    # Record the progress of training\n",
        "                    train_writer.add_summary(summary, iteration)\n",
        "                    \n",
        "                    iteration += 1\n",
        "                    pbar.update(batch_size)\n",
        "            \n",
        "            # Average the training loss and accuracy of each epoch\n",
        "            avg_train_loss = np.mean(train_loss)\n",
        "            avg_train_acc = np.mean(train_acc) \n",
        "\n",
        "            val_state = sess.run(model.initial_state)\n",
        "            with tqdm(total=len(x_valid)) as pbar:\n",
        "                for x, y in get_batches(x_valid, y_valid, batch_size):\n",
        "                    feed = {model.inputs: x,\n",
        "                            model.labels: y[:, None],\n",
        "                            model.keep_prob: 1,\n",
        "                            model.initial_state: val_state}\n",
        "                    summary, batch_loss, batch_acc, val_state = sess.run([model.merged, \n",
        "                                                                          model.cost, \n",
        "                                                                          model.accuracy, \n",
        "                                                                          model.final_state], \n",
        "                                                                         feed_dict=feed)\n",
        "                    \n",
        "                    # Record the validation loss and accuracy of each epoch\n",
        "                    val_loss.append(batch_loss)\n",
        "                    val_acc.append(batch_acc)\n",
        "                    pbar.update(batch_size)\n",
        "            \n",
        "            # Average the validation loss and accuracy of each epoch\n",
        "            avg_valid_loss = np.mean(val_loss)    \n",
        "            avg_valid_acc = np.mean(val_acc)\n",
        "            valid_loss_summary.append(avg_valid_loss)\n",
        "            \n",
        "            # Record the validation data's progress\n",
        "            valid_writer.add_summary(summary, iteration)\n",
        "\n",
        "            # Print the progress of each epoch\n",
        "            print(\"Epoch: {}/{}\".format(e, epochs),\n",
        "                  \"Train Loss: {:.3f}\".format(avg_train_loss),\n",
        "                  \"Train Acc: {:.3f}\".format(avg_train_acc),\n",
        "                  \"Valid Loss: {:.3f}\".format(avg_valid_loss),\n",
        "                  \"Valid Acc: {:.3f}\".format(avg_valid_acc))\n",
        "\n",
        "            # Stop training if the validation loss does not decrease after 3 epochs\n",
        "            if avg_valid_loss > min(valid_loss_summary):\n",
        "                print(\"No Improvement.\")\n",
        "                stop_early += 1\n",
        "                if stop_early == 3:\n",
        "                    break   \n",
        "            \n",
        "            # Reset stop_early if the validation loss finds a new low\n",
        "            # Save a checkpoint of the model\n",
        "            else:\n",
        "                print(\"New Record!\")\n",
        "                stop_early = 0\n",
        "                checkpoint = \"./Review/Model/sentiment_{}.ckpt\".format(log_string)\n",
        "                saver.save(sess, checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlczf_KnWF7n",
        "colab_type": "code",
        "outputId": "2fa31869-0daf-41e3-8a26-1b63ba23d273",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The default parameters of the model\n",
        "n_words = len(word_index)\n",
        "print(n_words)\n",
        "embed_size = 300\n",
        "batch_size = 250\n",
        "lstm_size = 128\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "learning_rate = 0.001\n",
        "epochs = 13 #100\n",
        "multiple_fc = False\n",
        "fc_units = 256\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjuFEQXtWF7o",
        "colab_type": "code",
        "outputId": "5adba9c7-374e-4140-fd7c-683340350eea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "# Train the model with the desired tuning parameters\n",
        "for lstm_size in [64,128]:\n",
        "    for multiple_fc in [True, False]:\n",
        "        for fc_units in [128, 256]:\n",
        "            log_string = 'ru={},fcl={},fcu={}'.format(lstm_size,\n",
        "                                                      multiple_fc,\n",
        "                                                      fc_units)\n",
        "            model = build_rnn(n_words = n_words, \n",
        "                              embed_size = embed_size,\n",
        "                              batch_size = batch_size,\n",
        "                              lstm_size = lstm_size,\n",
        "                              num_layers = num_layers,\n",
        "                              dropout = dropout,\n",
        "                              learning_rate = learning_rate,\n",
        "                              multiple_fc = multiple_fc,\n",
        "                              fc_units = fc_units)            \n",
        "            train(model, epochs, log_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model: ru=64,fcl=True,fcu=128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21250/21250 [02:00<00:00, 178.95it/s]\n",
            "100%|██████████| 3750/3750 [00:06<00:00, 557.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/1 Train Loss: 0.245 Train Acc: 0.569 Valid Loss: 0.190 Valid Acc: 0.732\n",
            "New Record!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/21250 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model: ru=64,fcl=True,fcu=256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21250/21250 [01:59<00:00, 177.79it/s]\n",
            "100%|██████████| 3750/3750 [00:06<00:00, 552.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/1 Train Loss: 0.229 Train Acc: 0.626 Valid Loss: 0.162 Valid Acc: 0.779\n",
            "New Record!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/21250 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model: ru=64,fcl=False,fcu=128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21250/21250 [01:59<00:00, 177.46it/s]\n",
            "100%|██████████| 3750/3750 [00:06<00:00, 556.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/1 Train Loss: 0.225 Train Acc: 0.636 Valid Loss: 0.190 Valid Acc: 0.726\n",
            "New Record!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/21250 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model: ru=64,fcl=False,fcu=256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21250/21250 [01:58<00:00, 179.10it/s]\n",
            "100%|██████████| 3750/3750 [00:06<00:00, 554.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/1 Train Loss: 0.230 Train Acc: 0.638 Valid Loss: 0.171 Valid Acc: 0.762\n",
            "New Record!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/21250 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model: ru=128,fcl=True,fcu=128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21250/21250 [03:31<00:00, 101.18it/s]\n",
            "100%|██████████| 3750/3750 [00:11<00:00, 324.73it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/1 Train Loss: 0.224 Train Acc: 0.637 Valid Loss: 0.224 Valid Acc: 0.647\n",
            "New Record!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/21250 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model: ru=128,fcl=True,fcu=256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21250/21250 [03:30<00:00, 100.42it/s]\n",
            "100%|██████████| 3750/3750 [00:11<00:00, 326.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/1 Train Loss: 0.218 Train Acc: 0.656 Valid Loss: 0.145 Valid Acc: 0.803\n",
            "New Record!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/21250 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model: ru=128,fcl=False,fcu=128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21250/21250 [03:28<00:00, 101.55it/s]\n",
            "100%|██████████| 3750/3750 [00:11<00:00, 323.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/1 Train Loss: 0.200 Train Acc: 0.688 Valid Loss: 0.138 Valid Acc: 0.807\n",
            "New Record!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/21250 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model: ru=128,fcl=False,fcu=256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21250/21250 [03:29<00:00, 101.47it/s]\n",
            "100%|██████████| 3750/3750 [00:11<00:00, 324.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/1 Train Loss: 0.205 Train Acc: 0.689 Valid Loss: 0.155 Valid Acc: 0.788\n",
            "New Record!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBt3OH1AWF7s",
        "colab_type": "text"
      },
      "source": [
        "# Make the Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD0lhir8WF7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_predictions(lstm_size, multiple_fc, fc_units, checkpoint):\n",
        "    '''Predict the sentiment of the testing data'''\n",
        "    \n",
        "    # Record all of the predictions\n",
        "    all_preds = []\n",
        "\n",
        "    model = build_rnn(n_words = n_words, \n",
        "                      embed_size = embed_size,\n",
        "                      batch_size = batch_size,\n",
        "                      lstm_size = lstm_size,\n",
        "                      num_layers = num_layers,\n",
        "                      dropout = dropout,\n",
        "                      learning_rate = learning_rate,\n",
        "                      multiple_fc = multiple_fc,\n",
        "                      fc_units = fc_units) \n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        saver = tf.train.Saver()\n",
        "        # Load the model\n",
        "        saver.restore(sess, checkpoint)\n",
        "        test_state = sess.run(model.initial_state)\n",
        "        for _, x in enumerate(get_test_batches(x_test, batch_size), 1):\n",
        "            feed = {model.inputs: x,\n",
        "                    model.keep_prob: 1,\n",
        "                    model.initial_state: test_state}\n",
        "            predictions = sess.run(model.predictions, feed_dict=feed)\n",
        "            \n",
        "            #for pred in predictions:\n",
        "                #all_preds.append(float(pred))\n",
        "                #print(float(pred))\n",
        "                \n",
        "    return all_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofz-0caoWF7w",
        "colab_type": "text"
      },
      "source": [
        "I am going to compare the results of the best three models, based on the validation data. Then average the predictions of these three models, which should produce an even better set of predictions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM28FrhWWF7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint1 = \"./Review/Model/sentiment_ru=128,fcl=False,fcu=256.ckpt\"\n",
        "checkpoint2 = \"./Review/Model/sentiment_ru=128,fcl=False,fcu=128.ckpt\"\n",
        "checkpoint3 = \"./Review/Model/sentiment_ru=64,fcl=True,fcu=256.ckpt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yjhN3utWF7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make predictions using the best 3 models\n",
        "predictions1 = make_predictions(128, False, 256, checkpoint1)\n",
        "predictions2 = make_predictions(128, False, 128, checkpoint2)\n",
        "predictions3 = make_predictions(64, True, 256, checkpoint3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx1Q6_33WF70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Average the best three predictions\n",
        "predictions_combined = (pd.DataFrame(predictions1) + pd.DataFrame(predictions2) + pd.DataFrame(predictions3))/3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "KzMveBh0WF75",
        "colab_type": "text"
      },
      "source": [
        "The results of the predictions are as follows:\n",
        "- Predictions1: 0.919\n",
        "- Predictions2: 0.914\n",
        "- Predictions3: 0.916\n",
        "- Combined Predictions: 0.935"
      ]
    }
  ]
}